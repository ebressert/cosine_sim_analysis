{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer1 = nn.Linear(embedding_dim, embedding_dim * 2)\n",
    "        self.layer2 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        pooled = torch.mean(embedded, dim=1)\n",
    "        x = F.relu(self.layer1(pooled))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_similarity(self, x1, x2):\n",
    "        embedded1 = self.embedding(x1)\n",
    "        embedded2 = self.embedding(x2)\n",
    "        return F.cosine_similarity(embedded1.mean(1), embedded2.mean(1))\n",
    "\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=4, batch_first=True, dropout=0.1)\n",
    "        self.layer1 = nn.Linear(embedding_dim, embedding_dim * 2)\n",
    "        self.layer2 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attended, _ = self.attention(embedded, embedded, embedded)\n",
    "        attended = self.layer_norm(attended + embedded)  # Residual connection\n",
    "        pooled = torch.mean(attended, dim=1)\n",
    "        x = F.relu(self.layer1(pooled))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_similarity(self, x1, x2):\n",
    "        e1, e2 = self.embedding(x1), self.embedding(x2)\n",
    "        attn_output, _ = self.attention(e1, e2, e2)\n",
    "        return torch.sum(attn_output * e1, dim=-1) / torch.sqrt(torch.tensor(self.embedding.embedding_dim))\n",
    "\n",
    "def train_and_measure(model, train_loader, val_loader, test_loader, epochs=30):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    metrics = defaultdict(list)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    max_patience = 5\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_energy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                val_loss += F.cross_entropy(output, target).item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                val_correct += pred.eq(target).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['train_loss'].append(epoch_loss / len(train_loader))\n",
    "        metrics['val_loss'].append(avg_val_loss)\n",
    "        metrics['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss: {metrics[\"train_loss\"][-1]:.4f}, '\n",
    "              f'Val Loss: {metrics[\"val_loss\"][-1]:.4f}, '\n",
    "              f'Val Accuracy: {metrics[\"val_accuracy\"][-1]:.4f}')\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            test_correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'training_time': training_time,\n",
    "        'energy_proxy': total_energy,\n",
    "        'test_loss': test_loss / len(test_loader),\n",
    "        'test_accuracy': test_correct / len(test_loader.dataset)\n",
    "    }\n",
    "\n",
    "def run_comparison(vocab_size=1000, embedding_dim=64, dataset_size=10000):\n",
    "    # Generate even more structured synthetic data\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(dataset_size):\n",
    "        pattern_type = np.random.random()\n",
    "        \n",
    "        if pattern_type < 0.33:\n",
    "            # Pattern 1: sum of first and last token maps to label\n",
    "            seq = torch.randint(0, vocab_size//4, (10,))\n",
    "            label = (seq[0] + seq[-1]) % vocab_size\n",
    "            \n",
    "        elif pattern_type < 0.66:\n",
    "            # Pattern 2: sequence of similar tokens predicts next\n",
    "            base_token = torch.randint(0, vocab_size//4, (1,))\n",
    "            noise = torch.randint(-2, 3, (10,))\n",
    "            seq = (base_token + noise) % vocab_size\n",
    "            label = (base_token + 3) % vocab_size\n",
    "            \n",
    "        else:\n",
    "            # Pattern 3: majority token in sequence\n",
    "            majority_token = torch.randint(0, vocab_size//4, (1,))\n",
    "            seq = torch.randint(0, vocab_size//4, (10,))\n",
    "            # Insert majority token multiple times\n",
    "            positions = torch.randint(0, 10, (4,))\n",
    "            seq[positions] = majority_token\n",
    "            label = majority_token\n",
    "\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    data = torch.stack(sequences)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Split into train/validation/test\n",
    "    train_size = int(0.7 * len(data))\n",
    "    val_size = int(0.15 * len(data))\n",
    "    \n",
    "    train_data = DataLoader(\n",
    "        list(zip(data[:train_size], labels[:train_size])), \n",
    "        batch_size=32,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_data = DataLoader(\n",
    "        list(zip(data[train_size:train_size+val_size], \n",
    "                labels[train_size:train_size+val_size])), \n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    test_data = DataLoader(\n",
    "        list(zip(data[train_size+val_size:], labels[train_size+val_size:])), \n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Train both models with validation\n",
    "    base_model = BaseEmbeddingModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "    cross_attn_model = CrossAttentionModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "    \n",
    "    base_results = train_and_measure(base_model, train_data, val_data, test_data, epochs=30)\n",
    "    cross_attn_results = train_and_measure(cross_attn_model, train_data, val_data, test_data, epochs=30)\n",
    "    \n",
    "    return {\n",
    "        'base_model': base_results,\n",
    "        'cross_attention': cross_attn_results\n",
    "    }\n",
    "\n",
    "def print_results(results, configuration):\n",
    "    print(f\"\\n=== Results for {configuration} ===\")\n",
    "    print(\"\\nBase Model:\")\n",
    "    print(f\"Training Time: {results['base_model']['training_time']:.2f} seconds\")\n",
    "    print(f\"Final Training Loss: {results['base_model']['metrics']['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss: {results['base_model']['metrics']['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Test Loss: {results['base_model']['test_loss']:.4f}\")\n",
    "    print(f\"Final Test Accuracy: {results['base_model']['test_accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"\\nCross-Attention Model:\")\n",
    "    print(f\"Training Time: {results['cross_attention']['training_time']:.2f} seconds\")\n",
    "    print(f\"Final Training Loss: {results['cross_attention']['metrics']['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss: {results['cross_attention']['metrics']['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Test Loss: {results['cross_attention']['test_loss']:.4f}\")\n",
    "    print(f\"Final Test Accuracy: {results['cross_attention']['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.7187, Val Loss: 3.0403, Val Accuracy: 0.1693\n",
      "Epoch 2: Train Loss: 2.8727, Val Loss: 2.6366, Val Accuracy: 0.2680\n",
      "Epoch 3: Train Loss: 2.6037, Val Loss: 2.4737, Val Accuracy: 0.3053\n",
      "Epoch 4: Train Loss: 2.4515, Val Loss: 2.3371, Val Accuracy: 0.3653\n",
      "Epoch 5: Train Loss: 2.3229, Val Loss: 2.2285, Val Accuracy: 0.4027\n",
      "Epoch 6: Train Loss: 2.2405, Val Loss: 2.1598, Val Accuracy: 0.4320\n",
      "Epoch 7: Train Loss: 2.1505, Val Loss: 2.0703, Val Accuracy: 0.4760\n",
      "Epoch 8: Train Loss: 2.0874, Val Loss: 2.0234, Val Accuracy: 0.5107\n",
      "Epoch 9: Train Loss: 2.0126, Val Loss: 1.9667, Val Accuracy: 0.5267\n",
      "Epoch 10: Train Loss: 1.9619, Val Loss: 1.9221, Val Accuracy: 0.5507\n",
      "Epoch 11: Train Loss: 1.9122, Val Loss: 1.8965, Val Accuracy: 0.5560\n",
      "Epoch 12: Train Loss: 1.8954, Val Loss: 1.8527, Val Accuracy: 0.5773\n",
      "Epoch 13: Train Loss: 1.8390, Val Loss: 1.8339, Val Accuracy: 0.5720\n",
      "Epoch 14: Train Loss: 1.8052, Val Loss: 1.8117, Val Accuracy: 0.5733\n",
      "Epoch 15: Train Loss: 1.7752, Val Loss: 1.8010, Val Accuracy: 0.5747\n",
      "Epoch 16: Train Loss: 1.7823, Val Loss: 1.7852, Val Accuracy: 0.5840\n",
      "Epoch 17: Train Loss: 1.7501, Val Loss: 1.7692, Val Accuracy: 0.5907\n",
      "Epoch 18: Train Loss: 1.7303, Val Loss: 1.7655, Val Accuracy: 0.5853\n",
      "Epoch 19: Train Loss: 1.7130, Val Loss: 1.7524, Val Accuracy: 0.5787\n",
      "Epoch 20: Train Loss: 1.6819, Val Loss: 1.7431, Val Accuracy: 0.5787\n",
      "Epoch 21: Train Loss: 1.6587, Val Loss: 1.7419, Val Accuracy: 0.5773\n",
      "Epoch 22: Train Loss: 1.6638, Val Loss: 1.7482, Val Accuracy: 0.5773\n",
      "Epoch 23: Train Loss: 1.6605, Val Loss: 1.7217, Val Accuracy: 0.5840\n",
      "Epoch 24: Train Loss: 1.6293, Val Loss: 1.7303, Val Accuracy: 0.5893\n",
      "Epoch 25: Train Loss: 1.6226, Val Loss: 1.7199, Val Accuracy: 0.5907\n",
      "Epoch 26: Train Loss: 1.5998, Val Loss: 1.7137, Val Accuracy: 0.5973\n",
      "Epoch 27: Train Loss: 1.6018, Val Loss: 1.7256, Val Accuracy: 0.5893\n",
      "Epoch 28: Train Loss: 1.5861, Val Loss: 1.7173, Val Accuracy: 0.6027\n",
      "Epoch 29: Train Loss: 1.5724, Val Loss: 1.7138, Val Accuracy: 0.6000\n",
      "Epoch 30: Train Loss: 1.5538, Val Loss: 1.7139, Val Accuracy: 0.5933\n",
      "Epoch 1: Train Loss: 3.7476, Val Loss: 3.0664, Val Accuracy: 0.1413\n",
      "Epoch 2: Train Loss: 2.9208, Val Loss: 2.6535, Val Accuracy: 0.2320\n",
      "Epoch 3: Train Loss: 2.6516, Val Loss: 2.4497, Val Accuracy: 0.3200\n",
      "Epoch 4: Train Loss: 2.4438, Val Loss: 2.2499, Val Accuracy: 0.3813\n",
      "Epoch 5: Train Loss: 2.2659, Val Loss: 2.0953, Val Accuracy: 0.4587\n",
      "Epoch 6: Train Loss: 2.1373, Val Loss: 2.0218, Val Accuracy: 0.4827\n",
      "Epoch 7: Train Loss: 2.0336, Val Loss: 1.9309, Val Accuracy: 0.5133\n",
      "Epoch 8: Train Loss: 1.9480, Val Loss: 1.8824, Val Accuracy: 0.5413\n",
      "Epoch 9: Train Loss: 1.8844, Val Loss: 1.8274, Val Accuracy: 0.5693\n",
      "Epoch 10: Train Loss: 1.8107, Val Loss: 1.7705, Val Accuracy: 0.5880\n",
      "Epoch 11: Train Loss: 1.7782, Val Loss: 1.7559, Val Accuracy: 0.5733\n",
      "Epoch 12: Train Loss: 1.7221, Val Loss: 1.7492, Val Accuracy: 0.5840\n",
      "Epoch 13: Train Loss: 1.7231, Val Loss: 1.7448, Val Accuracy: 0.5867\n",
      "Epoch 14: Train Loss: 1.6691, Val Loss: 1.7261, Val Accuracy: 0.5947\n",
      "Epoch 15: Train Loss: 1.6407, Val Loss: 1.7342, Val Accuracy: 0.5987\n",
      "Epoch 16: Train Loss: 1.6142, Val Loss: 1.7439, Val Accuracy: 0.5920\n",
      "Epoch 17: Train Loss: 1.6112, Val Loss: 1.6963, Val Accuracy: 0.6000\n",
      "Epoch 18: Train Loss: 1.5795, Val Loss: 1.7037, Val Accuracy: 0.6093\n",
      "Epoch 19: Train Loss: 1.5537, Val Loss: 1.7318, Val Accuracy: 0.5960\n",
      "Epoch 20: Train Loss: 1.5410, Val Loss: 1.7234, Val Accuracy: 0.6013\n",
      "Epoch 21: Train Loss: 1.5313, Val Loss: 1.6954, Val Accuracy: 0.6093\n",
      "Epoch 22: Train Loss: 1.5171, Val Loss: 1.7234, Val Accuracy: 0.6067\n",
      "Epoch 23: Train Loss: 1.4878, Val Loss: 1.7375, Val Accuracy: 0.6053\n",
      "Epoch 24: Train Loss: 1.4749, Val Loss: 1.7310, Val Accuracy: 0.6093\n",
      "Epoch 25: Train Loss: 1.4773, Val Loss: 1.7104, Val Accuracy: 0.6027\n",
      "Early stopping at epoch 26\n",
      "\n",
      "=== Results for Enhanced Configuration ===\n",
      "\n",
      "Base Model:\n",
      "Training Time: 2.15 seconds\n",
      "Final Training Loss: 1.5538\n",
      "Final Validation Loss: 1.7139\n",
      "Final Test Loss: 1.7296\n",
      "Final Test Accuracy: 0.6013\n",
      "\n",
      "Cross-Attention Model:\n",
      "Training Time: 7.26 seconds\n",
      "Final Training Loss: 1.4773\n",
      "Final Validation Loss: 1.7104\n",
      "Final Test Loss: 1.7724\n",
      "Final Test Accuracy: 0.6173\n"
     ]
    }
   ],
   "source": [
    "# Run with slightly larger settings\n",
    "results = run_comparison(\n",
    "    vocab_size=100,  # Increased vocabulary\n",
    "    embedding_dim=64,  # Increased embedding dimension\n",
    "    dataset_size=5000  # Increased dataset size\n",
    ")\n",
    "\n",
    "print_results(results, \"Enhanced Configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
