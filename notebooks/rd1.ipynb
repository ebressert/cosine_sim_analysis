{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer1 = nn.Linear(embedding_dim, embedding_dim * 2)\n",
    "        self.layer2 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        pooled = torch.mean(embedded, dim=1)\n",
    "        x = F.relu(self.layer1(pooled))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_similarity(self, x1, x2):\n",
    "        embedded1 = self.embedding(x1)\n",
    "        embedded2 = self.embedding(x2)\n",
    "        return F.cosine_similarity(embedded1.mean(1), embedded2.mean(1))\n",
    "\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=4, batch_first=True, dropout=0.1)\n",
    "        self.layer1 = nn.Linear(embedding_dim, embedding_dim * 2)\n",
    "        self.layer2 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attended, _ = self.attention(embedded, embedded, embedded)\n",
    "        attended = self.layer_norm(attended + embedded)  # Residual connection\n",
    "        pooled = torch.mean(attended, dim=1)\n",
    "        x = F.relu(self.layer1(pooled))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_similarity(self, x1, x2):\n",
    "        e1, e2 = self.embedding(x1), self.embedding(x2)\n",
    "        attn_output, _ = self.attention(e1, e2, e2)\n",
    "        return torch.sum(attn_output * e1, dim=-1) / torch.sqrt(torch.tensor(self.embedding.embedding_dim))\n",
    "\n",
    "def run_comparison(vocab_size=1000, embedding_dim=64, dataset_size=10000):\n",
    "    # Generate more structured synthetic data\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    # Create patterns in the data\n",
    "    for _ in range(dataset_size):\n",
    "        if np.random.random() < 0.5:\n",
    "            # Pattern 1: sum of first and last token maps to label\n",
    "            seq = torch.randint(0, vocab_size//4, (10,))\n",
    "            label = (seq[0] + seq[-1]) % vocab_size\n",
    "        else:\n",
    "            # Pattern 2: sequence of similar tokens predicts next\n",
    "            base_token = torch.randint(0, vocab_size//4, (1,))\n",
    "            noise = torch.randint(-2, 3, (10,))\n",
    "            seq = (base_token + noise) % vocab_size\n",
    "            label = (base_token + 3) % vocab_size\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    data = torch.stack(sequences)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_size = int(0.8 * len(data))\n",
    "    train_data = DataLoader(list(zip(data[:train_size], labels[:train_size])), \n",
    "                           batch_size=32,\n",
    "                           shuffle=True)\n",
    "    test_data = DataLoader(list(zip(data[train_size:], labels[train_size:])), \n",
    "                          batch_size=32)\n",
    "    \n",
    "    # Train both models\n",
    "    base_model = BaseEmbeddingModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "    cross_attn_model = CrossAttentionModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "    \n",
    "    base_results = train_and_measure(base_model, train_data, test_data, epochs=20)\n",
    "    cross_attn_results = train_and_measure(cross_attn_model, train_data, test_data, epochs=20)\n",
    "    \n",
    "    return {\n",
    "        'base_model': base_results,\n",
    "        'cross_attention': cross_attn_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3.6935, Test Loss: 3.2273, Accuracy: 0.0850\n",
      "Epoch 2: Train Loss: 2.8991, Test Loss: 2.6915, Accuracy: 0.2600\n",
      "Epoch 3: Train Loss: 2.5839, Test Loss: 2.4154, Accuracy: 0.3400\n",
      "Epoch 4: Train Loss: 2.3549, Test Loss: 2.2454, Accuracy: 0.3500\n",
      "Epoch 5: Train Loss: 2.2396, Test Loss: 2.1354, Accuracy: 0.3775\n",
      "Epoch 6: Train Loss: 2.1410, Test Loss: 2.0781, Accuracy: 0.4150\n",
      "Epoch 7: Train Loss: 2.1149, Test Loss: 2.0210, Accuracy: 0.4525\n",
      "Epoch 8: Train Loss: 2.0615, Test Loss: 1.9917, Accuracy: 0.4375\n",
      "Epoch 9: Train Loss: 1.9952, Test Loss: 1.9608, Accuracy: 0.4475\n",
      "Epoch 10: Train Loss: 1.9930, Test Loss: 1.9396, Accuracy: 0.4525\n",
      "Epoch 11: Train Loss: 1.9729, Test Loss: 1.9087, Accuracy: 0.4600\n",
      "Epoch 12: Train Loss: 1.9281, Test Loss: 1.8795, Accuracy: 0.4700\n",
      "Epoch 13: Train Loss: 1.8945, Test Loss: 1.8616, Accuracy: 0.4625\n",
      "Epoch 14: Train Loss: 1.8736, Test Loss: 1.8357, Accuracy: 0.4700\n",
      "Epoch 15: Train Loss: 1.8564, Test Loss: 1.8241, Accuracy: 0.4775\n",
      "Epoch 16: Train Loss: 1.8503, Test Loss: 1.8121, Accuracy: 0.4700\n",
      "Epoch 17: Train Loss: 1.8122, Test Loss: 1.8087, Accuracy: 0.4625\n",
      "Epoch 18: Train Loss: 1.7976, Test Loss: 1.7926, Accuracy: 0.4700\n",
      "Epoch 19: Train Loss: 1.8098, Test Loss: 1.7819, Accuracy: 0.4725\n",
      "Epoch 20: Train Loss: 1.7881, Test Loss: 1.7766, Accuracy: 0.4575\n",
      "Epoch 1: Train Loss: 3.6496, Test Loss: 2.9610, Accuracy: 0.0925\n",
      "Epoch 2: Train Loss: 2.8077, Test Loss: 2.5486, Accuracy: 0.2475\n",
      "Epoch 3: Train Loss: 2.4733, Test Loss: 2.2471, Accuracy: 0.3375\n",
      "Epoch 4: Train Loss: 2.2680, Test Loss: 2.1119, Accuracy: 0.3875\n",
      "Epoch 5: Train Loss: 2.1432, Test Loss: 2.0064, Accuracy: 0.4500\n",
      "Epoch 6: Train Loss: 2.0592, Test Loss: 1.9312, Accuracy: 0.4700\n",
      "Epoch 7: Train Loss: 1.9741, Test Loss: 1.8808, Accuracy: 0.4575\n",
      "Epoch 8: Train Loss: 1.9219, Test Loss: 1.8237, Accuracy: 0.4575\n",
      "Epoch 9: Train Loss: 1.8492, Test Loss: 1.7778, Accuracy: 0.4900\n",
      "Epoch 10: Train Loss: 1.7894, Test Loss: 1.7410, Accuracy: 0.4950\n",
      "Epoch 11: Train Loss: 1.7730, Test Loss: 1.7073, Accuracy: 0.4900\n",
      "Epoch 12: Train Loss: 1.7267, Test Loss: 1.6895, Accuracy: 0.4775\n",
      "Epoch 13: Train Loss: 1.6916, Test Loss: 1.6795, Accuracy: 0.4950\n",
      "Epoch 14: Train Loss: 1.6464, Test Loss: 1.6482, Accuracy: 0.4850\n",
      "Epoch 15: Train Loss: 1.6571, Test Loss: 1.6513, Accuracy: 0.4700\n",
      "Epoch 16: Train Loss: 1.6160, Test Loss: 1.6536, Accuracy: 0.4850\n",
      "Epoch 17: Train Loss: 1.6289, Test Loss: 1.6404, Accuracy: 0.4850\n",
      "Epoch 18: Train Loss: 1.6084, Test Loss: 1.6304, Accuracy: 0.4900\n",
      "Epoch 19: Train Loss: 1.5969, Test Loss: 1.6351, Accuracy: 0.4875\n",
      "Epoch 20: Train Loss: 1.5852, Test Loss: 1.6395, Accuracy: 0.4775\n",
      "\n",
      "=== Results for Improved Configuration ===\n",
      "\n",
      "Base Model:\n",
      "Training Time: 0.51 seconds\n",
      "Final Training Loss: 1.7881\n",
      "Final Test Loss: 1.7766\n",
      "Final Accuracy: 0.4575\n",
      "\n",
      "Cross-Attention Model:\n",
      "Training Time: 2.33 seconds\n",
      "Final Training Loss: 1.5852\n",
      "Final Test Loss: 1.6395\n",
      "Final Accuracy: 0.4775\n"
     ]
    }
   ],
   "source": [
    "# Run with smaller settings first\n",
    "results = run_comparison(\n",
    "    vocab_size=50,  # Even smaller vocabulary to start\n",
    "    embedding_dim=32,\n",
    "    dataset_size=2000\n",
    ")\n",
    "\n",
    "print_results(results, \"Improved Configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 4.2718, Test Loss: 3.9932, Accuracy: 0.0540\n",
      "Epoch 2: Train Loss: 3.8338, Test Loss: 3.7558, Accuracy: 0.0950\n",
      "Epoch 3: Train Loss: 3.6597, Test Loss: 3.6358, Accuracy: 0.1200\n",
      "Epoch 4: Train Loss: 3.5441, Test Loss: 3.5437, Accuracy: 0.1310\n",
      "Epoch 5: Train Loss: 3.4451, Test Loss: 3.4627, Accuracy: 0.1400\n",
      "Epoch 6: Train Loss: 3.3560, Test Loss: 3.3921, Accuracy: 0.1460\n",
      "Epoch 7: Train Loss: 3.2764, Test Loss: 3.3274, Accuracy: 0.1440\n",
      "Epoch 8: Train Loss: 3.2037, Test Loss: 3.2745, Accuracy: 0.1440\n",
      "Epoch 9: Train Loss: 3.1387, Test Loss: 3.2294, Accuracy: 0.1520\n",
      "Epoch 10: Train Loss: 3.0813, Test Loss: 3.1873, Accuracy: 0.1510\n",
      "Epoch 1: Train Loss: 4.0981, Test Loss: 3.8277, Accuracy: 0.0460\n",
      "Epoch 2: Train Loss: 3.7429, Test Loss: 3.6741, Accuracy: 0.1000\n",
      "Epoch 3: Train Loss: 3.5522, Test Loss: 3.5727, Accuracy: 0.1120\n",
      "Epoch 4: Train Loss: 3.4042, Test Loss: 3.5079, Accuracy: 0.1240\n",
      "Epoch 5: Train Loss: 3.2831, Test Loss: 3.4540, Accuracy: 0.1150\n",
      "Epoch 6: Train Loss: 3.1912, Test Loss: 3.4017, Accuracy: 0.1280\n",
      "Epoch 7: Train Loss: 3.1054, Test Loss: 3.3834, Accuracy: 0.1230\n",
      "Epoch 8: Train Loss: 3.0411, Test Loss: 3.3707, Accuracy: 0.1360\n",
      "Epoch 9: Train Loss: 2.9829, Test Loss: 3.3473, Accuracy: 0.1370\n",
      "Epoch 10: Train Loss: 2.9327, Test Loss: 3.3303, Accuracy: 0.1310\n",
      "\n",
      "=== Results for Default Configuration ===\n",
      "\n",
      "Base Model:\n",
      "Training Time: 0.66 seconds\n",
      "Final Training Loss: 3.0813\n",
      "Final Test Loss: 3.1873\n",
      "Final Accuracy: 0.1510\n",
      "\n",
      "Cross-Attention Model:\n",
      "Training Time: 3.07 seconds\n",
      "Final Training Loss: 2.9327\n",
      "Final Test Loss: 3.3303\n",
      "Final Accuracy: 0.1310\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the comparison\n",
    "def print_results(results, configuration):\n",
    "    print(f\"\\n=== Results for {configuration} ===\")\n",
    "    print(\"\\nBase Model:\")\n",
    "    print(f\"Training Time: {results['base_model']['training_time']:.2f} seconds\")\n",
    "    print(f\"Final Training Loss: {results['base_model']['metrics']['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Test Loss: {results['base_model']['metrics']['test_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Accuracy: {results['base_model']['metrics']['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nCross-Attention Model:\")\n",
    "    print(f\"Training Time: {results['cross_attention']['training_time']:.2f} seconds\")\n",
    "    print(f\"Final Training Loss: {results['cross_attention']['metrics']['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Test Loss: {results['cross_attention']['metrics']['test_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Accuracy: {results['cross_attention']['metrics']['accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Try with smaller vocabulary and dataset size\n",
    "results = run_comparison(\n",
    "    vocab_size=100,  # Smaller vocabulary\n",
    "    embedding_dim=32,  # Smaller embedding dimension\n",
    "    dataset_size=5000  # Smaller dataset\n",
    ")\n",
    "\n",
    "print_results(results, \"Default Configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comparison with default settings...\n",
      "\n",
      "Running comparison with smaller embedding dimension...\n",
      "\n",
      "=== Results for Default Configuration (64d) ===\n",
      "\n",
      "Base Model:\n",
      "Training Time: 0.60 seconds\n",
      "Final Training Loss: 6.4715\n",
      "Final Test Loss: 6.9586\n",
      "Final Accuracy: 0.0010\n",
      "\n",
      "Cross-Attention Model:\n",
      "Training Time: 2.78 seconds\n",
      "Final Training Loss: 5.5849\n",
      "Final Test Loss: 7.9746\n",
      "Final Accuracy: 0.0005\n",
      "\n",
      "=== Results for Small Embedding Configuration (32d) ===\n",
      "\n",
      "Base Model:\n",
      "Training Time: 0.48 seconds\n",
      "Final Training Loss: 6.6879\n",
      "Final Test Loss: 6.9316\n",
      "Final Accuracy: 0.0000\n",
      "\n",
      "Cross-Attention Model:\n",
      "Training Time: 2.35 seconds\n",
      "Final Training Loss: 6.3628\n",
      "Final Test Loss: 7.3269\n",
      "Final Accuracy: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Run with default settings\n",
    "print(\"Running comparison with default settings...\")\n",
    "results_default = run_comparison(\n",
    "    vocab_size=1000,\n",
    "    embedding_dim=64,\n",
    "    dataset_size=10000\n",
    ")\n",
    "\n",
    "# Run with smaller embedding dimension\n",
    "print(\"\\nRunning comparison with smaller embedding dimension...\")\n",
    "results_small = run_comparison(\n",
    "    vocab_size=1000,\n",
    "    embedding_dim=32,\n",
    "    dataset_size=10000\n",
    ")\n",
    "\n",
    "# Print comparative results\n",
    "def print_results(results, configuration):\n",
    "    print(f\"\\n=== Results for {configuration} ===\")\n",
    "    print(\"\\nBase Model:\")\n",
    "    print(f\"Training Time: {results['base_model']['training_time']:.2f} seconds\")\n",
    "    print(f\"Final Training Loss: {results['base_model']['metrics']['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Test Loss: {results['base_model']['metrics']['test_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Accuracy: {results['base_model']['metrics']['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nCross-Attention Model:\")\n",
    "    print(f\"Training Time: {results['cross_attention']['training_time']:.2f} seconds\")\n",
    "    print(f\"Final Training Loss: {results['cross_attention']['metrics']['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Test Loss: {results['cross_attention']['metrics']['test_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Accuracy: {results['cross_attention']['metrics']['accuracy'][-1]:.4f}\")\n",
    "\n",
    "print_results(results_default, \"Default Configuration (64d)\")\n",
    "print_results(results_small, \"Small Embedding Configuration (32d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
